---
title: 凤凰架构-不可变基础设施
categories:
  - 读书笔记
  - 技术
tags:
  - 凤凰架构
date: 2021-11-03
---

周志明《凤凰架构：构建可靠的大型分布式系统》
https://icyfenix.cn/

围绕“不可变基础设施”的相关话题，以容器、编排系统和服务网格的发展为主线，介绍虚拟化容器与服务网格是如何模糊掉软件与硬件之间的界限，如何在基础设施与通讯层面上帮助微服务隐藏复杂性，以此解决原本只能由程序员通过软件编程来解决的分布式问题。

<!-- more -->

## 1. 从微服务到云原生
### 云原生
云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。
这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。

### 不可变基础设施
而在云原生基金会定义的“云原生”概念中，“不可变基础设施”提升到了与微服务平级的重要程度。此时，它的内涵已经不再局限于只是方便运维、程序升级和部署的手段，而是升华为了向应用代码隐藏分布式架构复杂度、让分布式架构得以成为一种能够普遍推广的普适架构风格的必要前提。

### 虚拟化的目标与类型
容器的首要目标是让软件分发部署的过程，从传统的发布安装包、靠人工部署，转变为直接发布已经部署好的、包含整套运行环境的虚拟化镜像。

一个计算机软件要能够正确运行，需要通过以下三方面的兼容性来共同保障：
* ISA 兼容：目标机器指令集兼容性，比如 ARM 架构的计算机无法直接运行面向 x86 架构编译的程序。
* ABI 兼容：目标系统或者依赖库的二进制兼容性，比如 Windows 系统环境中无法直接运行 Linux 的程序，又比如 DirectX 12 的游戏无法运行在 DirectX 9 之上。
* 环境兼容：目标环境的兼容性，比如没有正确设置的配置文件、环境变量、注册中心、数据库地址、文件系统的权限等等，当任何一个环境因素出现错误，都会让你的程序无法正常运行。

### 虚拟化技术

* 指令集虚拟化：指令集虚拟化就是仿真，它提供了几乎完全不受局限的兼容性，甚至能做到直接在 Web 浏览器上运行完整操作系统这种令人惊讶的效果。但是，由于每条指令都要由软件来转换和模拟，它也是性能损失最大的虚拟化技术。
* 硬件抽象层虚拟化：即以软件或者直接通过硬件来模拟处理器、芯片组、内存、磁盘控制器、显卡等设备的工作过程。如果没有预设语境，一般人们所说的“虚拟机”就是指这一类虚拟化技术。
* 操作系统层虚拟化：不会提供真实的操作系统，而是会采用隔离手段，使得不同进程拥有独立的系统资源和资源配额，这样看起来它好像是独享了整个操作系统一般，但其实系统的内核仍然是被不同进程所共享的。操作系统层虚拟化的另一个名字，就是“容器化”。
* 运行库虚拟化：运行库虚拟化选择使用软件翻译的方法来模拟系统，它是以一个独立进程来代替操作系统内核，来提供目标软件运行所需的全部能力。如：WINE(在 Linux 下运行 Windows 程序的软件)。
* 语言层虚拟化：即由虚拟机将高级语言生成的中间代码，转换为目标机器可以直接执行的指令，代表为 Java 的 JVM 和.NET 的 CLR。

## 2. 容器化技术
容器技术和思想的起源：chroot 命令，这是计算机操作系统中最早的成规模的隔离技术。

### 隔离访问：namespaces
![namespaces](https://static001.geekbang.org/resource/image/cf/77/cff464f7a2c3b32e5c0194bae3b28177.jpg)

### 隔离资源：cgroups
![cgroups](https://static001.geekbang.org/resource/image/e4/1a/e4cab52f2143e17bfc300d62afef2a1a.jpg)

namespaces 和 cgroups 对资源访问与资源配额的隔离，它们不仅是容器化技术的基础，在现代 Linux 操作系统中也已经成为了无可或缺的基石。

### 封装系统：LXC VS 封装应用：Docker
Linux 容器（LinuX Containers，LXC），一种封装系统的轻量级虚拟机。以封装系统为出发点，如果仍然是按照先装系统再装软件的思路，就永远无法做到一两分钟甚至十几秒钟就构造出一个合乎要求的软件运行环境。
Docker，一种封装应用的技术手段，短短数年Docker就已经成为了软件开发、测试、分发、部署等各个环节都难以或缺的基础支撑。

为什么要用 Docker 而不是 LXC：跨机器的绿色部署、以应用为中心的封装、自动构建、多版本支持、组件重用、共享、工具生态。

### 开放容器交互标准（Open Container Initiative，OCI）
在 Docker 的主导和倡议下，多家公司联合制定了OCI，这是一个关于容器格式和运行时的规范文件，其中包含了运行时标准（runtime-spec ）、容器镜像标准（image-spec）和镜像分发标准（distribution-spec，分发标准还未正式发布）。

* 运行时标准定义了应该如何运行一个容器、如何管理容器的状态和生命周期、如何使用操作系统的底层特性（namespaces、cgroup、pivot_root 等）；
* 容器镜像标准规定了容器镜像的格式、配置、元数据的格式，你可以理解为对镜像的静态描述；
* 镜像分发标准则规定了镜像推送和拉取的网络交互过程。

#### runC / libcontainer
Docker 开源了自己用 Golang 开发的libcontainer，这是一个越过 LXC 直接操作 namespaces 和 cgroups 的核心模块，有了 libcontainer 以后，Docker 就能直接与系统内核打交道，不必依赖 LXC 来提供容器化隔离能力了。

为了符合 OCI 标准，Docker 将 libcontainer 独立出来，封装重构成 runC 项目，并捐献给了 Linux 基金会管理。

#### containerd
这是一个负责管理容器执行、分发、监控、网络、构建、日志等功能的核心模块，其内部会为每个容器运行时创建一个 containerd-shim 适配进程，默认与 runC 搭配工作，但也可以切换到其他 OCI Runtime 实现上（然而实际并没做到，最后 containerd 仍是紧密绑定于 runC）。

为了能够兼容所有符合标准的 OCI Runtime 实现，Docker 进一步重构了 Docker Daemon 子系统，把其中与运行时交互的部分抽象为了containerd 项目。2016年，Docker 把 containerd 捐献给了 CNCF 管理。

![Docker](https://static001.geekbang.org/resource/image/f7/9d/f77990ab74fc001a49997465d070709d.jpg)

### 封装集群：Kubernetes
它的前身是 Google 内部已经运行多年的集群管理系统 Borg，在 2014 年 6 月使用 Golang 完全重写后开源。

如果说以 Docker 为代表的容器引擎，是把软件的发布流程从分发二进制安装包，转变为了直接分发虚拟化后的整个运行环境，让应用得以实现跨机器的绿色部署；那以 Kubernetes 为代表的容器编排框架，就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，让集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩。

![Kubernetes](https://static001.geekbang.org/resource/image/d1/05/d1fb0e199a77f621c4034827160b5d05.jpg)

* kubelet：集群节点中的代理程序，负责与管理集群的 Master 通信。
* 容器运行时接口（Container Runtime Interface，CRI）：Kubernetes 1.5 版本开始引入，这是一个定义容器运行时应该如何接入到 kubelet 的规范标准，从此 Kubernetes 内部的 DockerManager，就被更为通用的 KubeGenericRuntimeManager 所替代了。
* DockerShim：由于 CRI 是在 Docker 之后才发布的规范，Docker 是肯定不支持 CRI 的，所以 Kubernetes 又提供了 DockerShim 服务作为 Docker 与 CRI 的适配层，由它与 Docker Engine 以 HTTP 形式通信，从而实现了原来 DockerManager 的全部功能。
* 容器运行时接口协调器（Container Runtime Interface Orchestrator，CRI-O）：完全遵循 CRI 规范来实现的，它可以支持所有符合 OCI 运行时标准的容器引擎，默认仍然是与 runC 搭配工作的。

#### Kubernetes 是如何一步步与 Docker 解耦的：
* 开源早起，Kubernetes 1.5 之前，通过DockerManager 向 Docker Engine 以 HTTP 方式发送指令：
Kubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC
* 2016年，Kubernetes 1.5 开始引入CRI，被更为通用的 KubeGenericRuntimeManager：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC
* 2017年，CRI-O发布，支持所有符合 OCI 运行时标准的容器引擎：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O → runC
* 2018年，containerd在CNCF孵化发布1.1版本，完美地支持了 CRI 标准：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC

## 3. 以容器构建系统

### Pod 的含义与职责
扮演容器组的角色，满足容器共享名称空间的需求，是 Pod 两大最基本的职责之一，同处于一个 Pod 内的多个容器，相互之间会以超亲密的方式协作。Pod 的另一个基本职责是实现原子性调度。
Pod 是隔离与调度的基本单位，也是我们接触的第一种 Kubernetes 资源。

超亲密的协作，是特指多个容器位于同一个 Pod 这种特殊关系，它们将默认共享以下名称空间，UTS 名称空间、网络名称空间、IPC 名称空间、时间名称空间，只有 PID 名称空间和文件名称空间默认是隔离的。

![Kubernetes的计算资源](https://static001.geekbang.org/resource/image/c8/5b/c8a5c9337c340cef5fcfd62c60f9815b.jpg)

* 容器（Container）：延续了自 Docker 以来一个容器封装一个应用进程的理念，是镜像管理的最小单位。
* 生产任务（Pod）：补充了容器化后缺失的与进程组对应的“容器组”的概念，Pod 中的容器共享 UTS、IPC、网络等名称空间，是资源调度的最小单位。
* 节点（Node）：对应于集群中的单台机器，这里的机器既可以是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位。
* 集群（Cluster）：对应于整个集群，Kubernetes 提倡的理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式 API 将你的意图写成一份元数据（Manifests），把它提交给集群即可，而无需关心它具体分配到哪个节点、如何实现 Pod 间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位。
* 集群联邦（Federation）：对应于多个集群，通过联邦可以统一管理多个 Kubernetes 集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求。

### 控制回路
![控制回路](https://static001.geekbang.org/resource/image/5e/33/5e3b11dc1ecb84c7d0b7dbac0c9cd733.jpg)

通过描述清楚这些资源的期望状态，由 Kubernetes 中对应监视这些资源的控制器，来驱动资源的实际状态逐渐向期望状态靠拢，才能够达成自己的目的，这种交互风格就被叫做 Kubernetes 的声明式 API。

资源与控制器是贯穿整个 Kubernetes 的两大设计理念。

Kubernetes 已内置支持相当多的资源对象，与资源相对应的，只要是实际状态有可能发生变化的资源对象，就通常都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源。

为了管理众多资源控制器，Kubernetes 设计了统一的控制器管理框架（kube-controller-manager）来维护这些控制器的正常运作，并设计了统一的指标监视器（kube-apiserver），用于在控制器工作时，为它提供追踪资源的度量数据。

## 4. 以应用为中心的封装

云原生基础设施的其中一个重要目标，是接管掉业务系统复杂的非功能特性，这会让业务研发与运维工作变得足够简单，不受分布式的牵绊。
然而，Kubernetes 被诟病得最多的就是复杂，不仅在于繁琐，还在于要想写出合适的元数据描述文件，你既需要懂开发，又需要懂运维，有时候还需要懂平台，一般企业根本找不到合适的角色来为它管理、部署和维护应用。以上提到的复杂性不能说是 Kubernetes 带来的，而是分布式架构本身的原罪。

### Kustomize 和 Helm
Kustomize 和 Helm，它们是封装“无状态应用”的典型代表。

#### Kustomize
Kustomize 的主要价值是根据环境来生成不同的部署配置。只要建立多个 Kustomization 文件，开发人员就能以基于基准进行派生（Base and Overlay）的方式，对不同的模式（比如生产模式、调试模式）、不同的项目（同一个产品对不同客户的客制化）定制出不同的资源整合包。

Kustomize 只能简化产品针对不同情况的重复配置，它其实并没有真正解决应用管理复杂的问题，要做的事、要写的配置，最终都没有减少，只是不用反复去写罢了。

#### Helm
如果说 Kubernetes 是云原生操作系统的话，那 Helm 就要成为这个操作系统上面的应用商店与包管理工具。

Helm 模拟的 Linux 下的包管理工具和封装格式，它提出了与 Linux 包管理直接对应的 Chart 格式和 Repository 应用仓库，另外针对 Kubernetes 中特有的一个应用经常要部署多个版本的特点，也提出了 Release 的专有概念。

Helm 提供了应用全生命周期、版本、依赖项的管理能力，同时，Helm 还支持额外的扩展插件，能够加入 CI/CD 或者其他方面的辅助功能

### Operator 和 OAM
有状态应用的两种封装方法，包括 Operator 和 OAM。

#### Operator
Operator不应当被称作是一种工具或者系统，它应该算是一种封装、部署和管理 Kubernetes 应用的方法，尤其是针对最复杂的有状态应用去封装运维能力的解决方案。

Operator 是通过 Kubernetes 1.7 开始支持的自定义资源（Custom Resource Definitions，CRD，此前曾经以 TPR，即 Third Party Resource 的形式提供过类似的能力），把应用封装为另一种更高层次的资源，再把 Kubernetes 的控制器模式从面向内置资源，扩展到了面向所有自定义资源，以此来完成对复杂应用的管理。

例如 Elasticsearch Operator 提供了一种kind: Elasticsearch的自定义资源，在它的帮助下，只需要十行代码，将用户的意图是“部署三个版本为 7.9.1 的 ES 集群节点”说清楚，就能实现跟前面 StatefulSet 那一大堆配置相同甚至是更强大的效果。

#### 开放应用模型（Open Application Model，OAM）
OAM 思想的核心是将开发人员、运维人员与平台人员的关注点分离，开发人员关注业务逻辑的实现，运维人员关注程序的平稳运行，平台人员关注基础设施的能力与稳定性。

OAM 对云原生应用的定义是：“由一组相互关联但又离散独立的组件构成，这些组件实例化在合适的运行时上，由配置来控制行为并共同协作提供统一的功能”。


## 5. Linux网络虚拟化

### Linux 系统下的网络通信模型
![Linux系统下的网络通信模型](https://static001.geekbang.org/resource/image/16/ab/16a2c5b7133827bc83f9af9c3d262dab.jpg)

* Socket：应用层的程序是通过 Socket 编程接口，来和内核空间的网络协议栈通信的。在这里，应用程序通过读写收、发缓冲区（Receive/Send Buffer）来与 Socket 进行交互。
* TCP/UDP：以 TCP 协议为例，内核发现 Socket 的发送缓冲区中，有新的数据被拷贝进来后，会把数据封装为 TCP Segment 报文，常见的网络协议的报文基本上都是由报文头（Header）和报文体（Body，也叫荷载“Payload”）两部分组成。接着，系统内核将缓冲区中用户要发送出去的数据作为报文体，然后把传输层中的必要控制信息，比如代表哪个程序发、由哪个程序收的源、目标端口号，用于保证可靠通信（重发与控制顺序）的序列号、用于校验信息是否在传输中出现损失的校验和（Check Sum）等信息，封装入报文头中。
* IP：它会把来自上一层（即前面例子中的 TCP 报文）的数据包作为报文体，然后再次加入到自己的报文头中，比如指明数据应该发到哪里的路由地址、数据包的长度、协议的版本号，等等，这样封装成 IP 数据包后再发往下一层。
* Device：Device 即网络设备，它是网络访问层中面向系统一侧的接口。不过这里所说的设备，跟物理硬件设备并不是同一个概念，Device 只是一种向操作系统端开放的接口，它的背后既可能代表着真实的物理硬件，也可能是某段具有特定功能的程序代码，比如即使不存在物理网卡，也依然可以存在回环设备（Loopback Device）。许多网络抓包工具，比如tcpdump、Wirshark就是在此处工作的。Device 主要的作用是抽象出统一的界面，让程序代码去选择或影响收发包出入口，比如决定数据应该从哪块网卡设备发送出去；还有就是准备好网卡驱动工作所需的数据。
* Driver：网卡驱动程序（Driver）是网络访问层中面向硬件一侧的接口，网卡驱动程序会通过DMA把主存中的待发送的数据包，复制到驱动内部的缓冲区之中。数据被复制的同时，也会把上层提供的 IP 数据包、下一跳的 MAC 地址这些信息，加上网卡的 MAC 地址、VLAN Tag 等信息，一并封装成为以太帧（Ethernet Frame），并自动计算校验和。而对于需要确认重发的信息，如果没有收到接收者的确认（ACK）响应，那重发的处理也是在这里自动完成的。

### 干预网络通信的 Netfilter 框架
![应用收、发数据包所经过的Netfilter钩子](https://static001.geekbang.org/resource/image/68/e5/688b390674c2de47ce398d2ab8d2a3e5.jpg)

* PREROUTING：来自设备的数据包进入协议栈后，就会立即触发这个钩子。注意，如果 PREROUTING 钩子在进入 IP 路由之前触发了，就意味着只要接收到的数据包，无论是否真的发往本机，也都会触发这个钩子。它一般是用于目标网络地址转换（Destination NAT，DNAT）。
* INPUT：报文经过 IP 路由后，如果确定是发往本机的，将会触发这个钩子，它一般用于加工发往本地进程的数据包。
* FORWARD：报文经过 IP 路由后，如果确定不是发往本机的，将会触发这个钩子，它一般用于处理转发到其他机器的数据包。
* OUTPUT：从本机程序发出的数据包，在经过 IP 路由前，将会触发这个钩子，它一般用于加工本地进程的输出数据包。
* POSTROUTING：从本机网卡出去的数据包，无论是本机的程序所发出的，还是由本机转发给其他机器的，都会触发这个钩子，它一般是用于源网络地址转换（Source NAT，SNAT）。

以 Netfilter 为基础的应用也有很多，其中使用最广泛的毫无疑问要数 Xtables 系列工具，比如iptables、ebtables、arptables、ip6tables，等等。

### 虚拟化网络设备

#### 网卡：tun/tap、veth
tun 和 tap 是两个相对独立的虚拟网络设备，其中 tap 模拟了以太网设备，操作二层数据包（以太帧），tun 则是模拟了网络层设备，操作三层数据包（IP 报文）

使用 tun/tap 设备的目的，其实是为了把来自协议栈的数据包，先交给某个打开了/dev/net/tun字符设备的用户进程处理后，再把数据包重新发回到链路中。这里你可以通俗地理解为，这块虚拟化网卡驱动一端连接着网络协议栈，另一端连接着用户态程序，而普通的网卡驱动则是一端连接着网络协议栈，另一端连接着物理网卡。

VPN 应用程序：
![VPN中数据流动示意图](https://static001.geekbang.org/resource/image/3b/ab/3b88a0148da9a84bd782a7e0a0c384ab.jpg?wh=1504*809)

使用 tun/tap 设备来传输数据需要经过两次协议栈，所以会不可避免地产生一定的性能损耗。

veth 实际上也不是一个设备，而是一对设备，因而它也常被称作 veth pair。我们要使用 veth，就必须在两个独立的网络名称空间中进行才有意义，因为 veth pair 是一端连着协议栈，另一端彼此相连的，在 veth 设备的其中一端输入数据，这些数据就会从设备的另一端原样不动地流出。
![veth pair工作示意图](https://static001.geekbang.org/resource/image/cd/67/cd90382e98805d560810b23f4f273367.jpg?wh=553*336)

两个容器之间采用 veth 通信，不需要反复多次经过网络协议栈，这就让 veth 比起 tap/tun 来说，具备了更好的性能，也让 veth pair 的实现变得十分简单。

#### 交换机：Linux Bridge
二层转发、泛洪、STP、MAC 学习、地址转发表，等等。
它与普通的物理交换机也还是有一点差别的，普通交换机只会单纯地做二层转发，Linux Bridge 却还支持把发给它自身的数据包，接入到主机的三层协议栈中。

![Linux Bridge构建单IP容器网络](https://static001.geekbang.org/resource/image/9b/af/9bb747d54574ee8cbd9c4e22d16124af.jpg?wh=682*534)

#### 网络：VXLAN
软件定义网络（Software Defined Network，SDN）的需求在云计算和分布式时代，就变得前所未有地迫切。SDN 的核心思路是在物理的网络之上，再构造一层虚拟化的网络，把控制平面和数据平面分离开来，实现流量的灵活控制，为核心网络及应用的创新提供良好的平台。

SDN 里，位于下层的物理网络被称为 Underlay，它着重解决网络的连通性与可管理性；位于上层的逻辑网络被称为 Overlay，它着重为应用提供与软件需求相符的传输服务和网络拓扑。以 VXLAN 为例，给你介绍 Overlay 网络的原理。

为了解决 VLAN 的两个明显的缺陷（VLAN Tag 的设计、跨数据中心传递），IETF 定义了 VXLAN 规范，这是三层虚拟化网络（Network Virtualization over Layer 3，NVO3）的标准技术规范之一，是一种典型的 Overlay 网络。

VXLAN 对网络基础设施的要求很低，不需要专门的硬件提供特别支持，只要三层可达的网络就能部署 VXLAN。
VXLAN 带来了很高的灵活性、扩展性和可管理性，VXLAN也带来了额外的复杂度和性能开销（传输效率的下降、传输性能的下降）

#### 副本网卡：MACVLAN
两个 VLAN 之间位于独立的广播域，是完全二层隔离的，要通信就只能通过三层设备。而最简单的三层通信就是靠单臂路由了。单臂路由那把路由器上的两个接口分别设置不同的 IP 地址，然后用两条网线分别连接到交换机上，也勉强算是一个解决办法，但 VLAN 最多可以支持 4096 个 VLAN，那如果要接四千多条网线就太离谱了。因此为了解决这个问题，802.1Q 规范中专门定义了子接口（Sub-Interface）的概念，它的作用是允许在同一张物理网卡上，针对不同的 VLAN 绑定不同的 IP 地址。

MACVLAN 就借用了 VLAN 子接口的思路，并且在这个基础上更进一步，不仅允许对同一个网卡设置多个 IP 地址，还允许对同一张网卡上设置多个 MAC 地址，这也是 MACVLAN 名字的由来。

![MACVLAN原理](https://static001.geekbang.org/resource/image/17/da/177318346dbc945526c064cf618f30da.jpg?wh=1696*1125)

虽然容器间的网络方案多种多样，但通信主体都是固定的，不外乎没有物理设备的虚拟主体（容器、Pod、Service、Endpoints 等等）、不需要跨网络的本地主机、以及通过网络连接的外部主机三种层次。所有的容器网络通信问题，其实都可以归结为本地主机内部的多个容器之间、本地主机与内部容器之间，以及跨越不同主机的多个容器之间的通信问题。