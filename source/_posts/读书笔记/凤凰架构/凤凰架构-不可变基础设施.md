---
title: 凤凰架构-不可变基础设施
categories:
  - 读书笔记
  - 技术
tags:
  - 凤凰架构
abbrlink: 1751902f
date: 2021-11-03 00:00:00
---

周志明《凤凰架构：构建可靠的大型分布式系统》
https://icyfenix.cn/

围绕“不可变基础设施”的相关话题，以容器、编排系统和服务网格的发展为主线，介绍虚拟化容器与服务网格是如何模糊掉软件与硬件之间的界限，如何在基础设施与通讯层面上帮助微服务隐藏复杂性，以此解决原本只能由程序员通过软件编程来解决的分布式问题。

<!-- more -->

## 1. 从微服务到云原生
### 云原生
云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。
这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。

### 不可变基础设施
而在云原生基金会定义的“云原生”概念中，“不可变基础设施”提升到了与微服务平级的重要程度。此时，它的内涵已经不再局限于只是方便运维、程序升级和部署的手段，而是升华为了向应用代码隐藏分布式架构复杂度、让分布式架构得以成为一种能够普遍推广的普适架构风格的必要前提。

### 虚拟化的目标与类型
容器的首要目标是让软件分发部署的过程，从传统的发布安装包、靠人工部署，转变为直接发布已经部署好的、包含整套运行环境的虚拟化镜像。

一个计算机软件要能够正确运行，需要通过以下三方面的兼容性来共同保障：
* ISA 兼容：目标机器指令集兼容性，比如 ARM 架构的计算机无法直接运行面向 x86 架构编译的程序。
* ABI 兼容：目标系统或者依赖库的二进制兼容性，比如 Windows 系统环境中无法直接运行 Linux 的程序，又比如 DirectX 12 的游戏无法运行在 DirectX 9 之上。
* 环境兼容：目标环境的兼容性，比如没有正确设置的配置文件、环境变量、注册中心、数据库地址、文件系统的权限等等，当任何一个环境因素出现错误，都会让你的程序无法正常运行。

### 虚拟化技术

* 指令集虚拟化：指令集虚拟化就是仿真，它提供了几乎完全不受局限的兼容性，甚至能做到直接在 Web 浏览器上运行完整操作系统这种令人惊讶的效果。但是，由于每条指令都要由软件来转换和模拟，它也是性能损失最大的虚拟化技术。
* 硬件抽象层虚拟化：即以软件或者直接通过硬件来模拟处理器、芯片组、内存、磁盘控制器、显卡等设备的工作过程。如果没有预设语境，一般人们所说的“虚拟机”就是指这一类虚拟化技术。
* 操作系统层虚拟化：不会提供真实的操作系统，而是会采用隔离手段，使得不同进程拥有独立的系统资源和资源配额，这样看起来它好像是独享了整个操作系统一般，但其实系统的内核仍然是被不同进程所共享的。操作系统层虚拟化的另一个名字，就是“容器化”。
* 运行库虚拟化：运行库虚拟化选择使用软件翻译的方法来模拟系统，它是以一个独立进程来代替操作系统内核，来提供目标软件运行所需的全部能力。如：WINE(在 Linux 下运行 Windows 程序的软件)。
* 语言层虚拟化：即由虚拟机将高级语言生成的中间代码，转换为目标机器可以直接执行的指令，代表为 Java 的 JVM 和.NET 的 CLR。

## 2. 容器化技术
容器技术和思想的起源：chroot 命令，这是计算机操作系统中最早的成规模的隔离技术。

### 隔离访问：namespaces
![namespaces](https://static001.geekbang.org/resource/image/cf/77/cff464f7a2c3b32e5c0194bae3b28177.jpg)

### 隔离资源：cgroups
![cgroups](https://static001.geekbang.org/resource/image/e4/1a/e4cab52f2143e17bfc300d62afef2a1a.jpg)

namespaces 和 cgroups 对资源访问与资源配额的隔离，它们不仅是容器化技术的基础，在现代 Linux 操作系统中也已经成为了无可或缺的基石。

### 封装系统：LXC VS 封装应用：Docker
Linux 容器（LinuX Containers，LXC），一种封装系统的轻量级虚拟机。以封装系统为出发点，如果仍然是按照先装系统再装软件的思路，就永远无法做到一两分钟甚至十几秒钟就构造出一个合乎要求的软件运行环境。
Docker，一种封装应用的技术手段，短短数年Docker就已经成为了软件开发、测试、分发、部署等各个环节都难以或缺的基础支撑。

为什么要用 Docker 而不是 LXC：跨机器的绿色部署、以应用为中心的封装、自动构建、多版本支持、组件重用、共享、工具生态。

### 开放容器交互标准（Open Container Initiative，OCI）
在 Docker 的主导和倡议下，多家公司联合制定了OCI，这是一个关于容器格式和运行时的规范文件，其中包含了运行时标准（runtime-spec ）、容器镜像标准（image-spec）和镜像分发标准（distribution-spec，分发标准还未正式发布）。

* 运行时标准定义了应该如何运行一个容器、如何管理容器的状态和生命周期、如何使用操作系统的底层特性（namespaces、cgroup、pivot_root 等）；
* 容器镜像标准规定了容器镜像的格式、配置、元数据的格式，你可以理解为对镜像的静态描述；
* 镜像分发标准则规定了镜像推送和拉取的网络交互过程。

#### runC / libcontainer
Docker 开源了自己用 Golang 开发的libcontainer，这是一个越过 LXC 直接操作 namespaces 和 cgroups 的核心模块，有了 libcontainer 以后，Docker 就能直接与系统内核打交道，不必依赖 LXC 来提供容器化隔离能力了。

为了符合 OCI 标准，Docker 将 libcontainer 独立出来，封装重构成 runC 项目，并捐献给了 Linux 基金会管理。

#### containerd
这是一个负责管理容器执行、分发、监控、网络、构建、日志等功能的核心模块，其内部会为每个容器运行时创建一个 containerd-shim 适配进程，默认与 runC 搭配工作，但也可以切换到其他 OCI Runtime 实现上（然而实际并没做到，最后 containerd 仍是紧密绑定于 runC）。

为了能够兼容所有符合标准的 OCI Runtime 实现，Docker 进一步重构了 Docker Daemon 子系统，把其中与运行时交互的部分抽象为了containerd 项目。2016年，Docker 把 containerd 捐献给了 CNCF 管理。

![Docker](https://static001.geekbang.org/resource/image/f7/9d/f77990ab74fc001a49997465d070709d.jpg)

### 封装集群：Kubernetes
它的前身是 Google 内部已经运行多年的集群管理系统 Borg，在 2014 年 6 月使用 Golang 完全重写后开源。

如果说以 Docker 为代表的容器引擎，是把软件的发布流程从分发二进制安装包，转变为了直接分发虚拟化后的整个运行环境，让应用得以实现跨机器的绿色部署；那以 Kubernetes 为代表的容器编排框架，就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，让集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩。

![Kubernetes](https://static001.geekbang.org/resource/image/d1/05/d1fb0e199a77f621c4034827160b5d05.jpg)

* kubelet：集群节点中的代理程序，负责与管理集群的 Master 通信。
* 容器运行时接口（Container Runtime Interface，CRI）：Kubernetes 1.5 版本开始引入，这是一个定义容器运行时应该如何接入到 kubelet 的规范标准，从此 Kubernetes 内部的 DockerManager，就被更为通用的 KubeGenericRuntimeManager 所替代了。
* DockerShim：由于 CRI 是在 Docker 之后才发布的规范，Docker 是肯定不支持 CRI 的，所以 Kubernetes 又提供了 DockerShim 服务作为 Docker 与 CRI 的适配层，由它与 Docker Engine 以 HTTP 形式通信，从而实现了原来 DockerManager 的全部功能。
* 容器运行时接口协调器（Container Runtime Interface Orchestrator，CRI-O）：完全遵循 CRI 规范来实现的，它可以支持所有符合 OCI 运行时标准的容器引擎，默认仍然是与 runC 搭配工作的。

#### Kubernetes 是如何一步步与 Docker 解耦的：
* 开源早起，Kubernetes 1.5 之前，通过DockerManager 向 Docker Engine 以 HTTP 方式发送指令：
Kubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC
* 2016年，Kubernetes 1.5 开始引入CRI，被更为通用的 KubeGenericRuntimeManager：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC
* 2017年，CRI-O发布，支持所有符合 OCI 运行时标准的容器引擎：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O → runC
* 2018年，containerd在CNCF孵化发布1.1版本，完美地支持了 CRI 标准：
Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC

## 3. 以容器构建系统

### Pod 的含义与职责
扮演容器组的角色，满足容器共享名称空间的需求，是 Pod 两大最基本的职责之一，同处于一个 Pod 内的多个容器，相互之间会以超亲密的方式协作。Pod 的另一个基本职责是实现原子性调度。
Pod 是隔离与调度的基本单位，也是我们接触的第一种 Kubernetes 资源。

超亲密的协作，是特指多个容器位于同一个 Pod 这种特殊关系，它们将默认共享以下名称空间，UTS 名称空间、网络名称空间、IPC 名称空间、时间名称空间，只有 PID 名称空间和文件名称空间默认是隔离的。

![Kubernetes的计算资源](https://static001.geekbang.org/resource/image/c8/5b/c8a5c9337c340cef5fcfd62c60f9815b.jpg)

* 容器（Container）：延续了自 Docker 以来一个容器封装一个应用进程的理念，是镜像管理的最小单位。
* 生产任务（Pod）：补充了容器化后缺失的与进程组对应的“容器组”的概念，Pod 中的容器共享 UTS、IPC、网络等名称空间，是资源调度的最小单位。
* 节点（Node）：对应于集群中的单台机器，这里的机器既可以是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位。
* 集群（Cluster）：对应于整个集群，Kubernetes 提倡的理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式 API 将你的意图写成一份元数据（Manifests），把它提交给集群即可，而无需关心它具体分配到哪个节点、如何实现 Pod 间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位。
* 集群联邦（Federation）：对应于多个集群，通过联邦可以统一管理多个 Kubernetes 集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求。

### 控制回路
![控制回路](https://static001.geekbang.org/resource/image/5e/33/5e3b11dc1ecb84c7d0b7dbac0c9cd733.jpg)

通过描述清楚这些资源的期望状态，由 Kubernetes 中对应监视这些资源的控制器，来驱动资源的实际状态逐渐向期望状态靠拢，才能够达成自己的目的，这种交互风格就被叫做 Kubernetes 的声明式 API。

资源与控制器是贯穿整个 Kubernetes 的两大设计理念。

Kubernetes 已内置支持相当多的资源对象，与资源相对应的，只要是实际状态有可能发生变化的资源对象，就通常都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源。

为了管理众多资源控制器，Kubernetes 设计了统一的控制器管理框架（kube-controller-manager）来维护这些控制器的正常运作，并设计了统一的指标监视器（kube-apiserver），用于在控制器工作时，为它提供追踪资源的度量数据。

## 4. 以应用为中心的封装

云原生基础设施的其中一个重要目标，是接管掉业务系统复杂的非功能特性，这会让业务研发与运维工作变得足够简单，不受分布式的牵绊。
然而，Kubernetes 被诟病得最多的就是复杂，不仅在于繁琐，还在于要想写出合适的元数据描述文件，你既需要懂开发，又需要懂运维，有时候还需要懂平台，一般企业根本找不到合适的角色来为它管理、部署和维护应用。以上提到的复杂性不能说是 Kubernetes 带来的，而是分布式架构本身的原罪。

### Kustomize 和 Helm
Kustomize 和 Helm，它们是封装“无状态应用”的典型代表。

#### Kustomize
Kustomize 的主要价值是根据环境来生成不同的部署配置。只要建立多个 Kustomization 文件，开发人员就能以基于基准进行派生（Base and Overlay）的方式，对不同的模式（比如生产模式、调试模式）、不同的项目（同一个产品对不同客户的客制化）定制出不同的资源整合包。

Kustomize 只能简化产品针对不同情况的重复配置，它其实并没有真正解决应用管理复杂的问题，要做的事、要写的配置，最终都没有减少，只是不用反复去写罢了。

#### Helm
如果说 Kubernetes 是云原生操作系统的话，那 Helm 就要成为这个操作系统上面的应用商店与包管理工具。

Helm 模拟的 Linux 下的包管理工具和封装格式，它提出了与 Linux 包管理直接对应的 Chart 格式和 Repository 应用仓库，另外针对 Kubernetes 中特有的一个应用经常要部署多个版本的特点，也提出了 Release 的专有概念。

Helm 提供了应用全生命周期、版本、依赖项的管理能力，同时，Helm 还支持额外的扩展插件，能够加入 CI/CD 或者其他方面的辅助功能

### Operator 和 OAM
有状态应用的两种封装方法，包括 Operator 和 OAM。

#### Operator
Operator不应当被称作是一种工具或者系统，它应该算是一种封装、部署和管理 Kubernetes 应用的方法，尤其是针对最复杂的有状态应用去封装运维能力的解决方案。

Operator 是通过 Kubernetes 1.7 开始支持的自定义资源（Custom Resource Definitions，CRD，此前曾经以 TPR，即 Third Party Resource 的形式提供过类似的能力），把应用封装为另一种更高层次的资源，再把 Kubernetes 的控制器模式从面向内置资源，扩展到了面向所有自定义资源，以此来完成对复杂应用的管理。

例如 Elasticsearch Operator 提供了一种kind: Elasticsearch的自定义资源，在它的帮助下，只需要十行代码，将用户的意图是“部署三个版本为 7.9.1 的 ES 集群节点”说清楚，就能实现跟前面 StatefulSet 那一大堆配置相同甚至是更强大的效果。

#### 开放应用模型（Open Application Model，OAM）
OAM 思想的核心是将开发人员、运维人员与平台人员的关注点分离，开发人员关注业务逻辑的实现，运维人员关注程序的平稳运行，平台人员关注基础设施的能力与稳定性。

OAM 对云原生应用的定义是：“由一组相互关联但又离散独立的组件构成，这些组件实例化在合适的运行时上，由配置来控制行为并共同协作提供统一的功能”。


## 5. Linux网络虚拟化

### Linux 系统下的网络通信模型
![Linux系统下的网络通信模型](https://static001.geekbang.org/resource/image/16/ab/16a2c5b7133827bc83f9af9c3d262dab.jpg)

* Socket：应用层的程序是通过 Socket 编程接口，来和内核空间的网络协议栈通信的。在这里，应用程序通过读写收、发缓冲区（Receive/Send Buffer）来与 Socket 进行交互。
* TCP/UDP：以 TCP 协议为例，内核发现 Socket 的发送缓冲区中，有新的数据被拷贝进来后，会把数据封装为 TCP Segment 报文，常见的网络协议的报文基本上都是由报文头（Header）和报文体（Body，也叫荷载“Payload”）两部分组成。接着，系统内核将缓冲区中用户要发送出去的数据作为报文体，然后把传输层中的必要控制信息，比如代表哪个程序发、由哪个程序收的源、目标端口号，用于保证可靠通信（重发与控制顺序）的序列号、用于校验信息是否在传输中出现损失的校验和（Check Sum）等信息，封装入报文头中。
* IP：它会把来自上一层（即前面例子中的 TCP 报文）的数据包作为报文体，然后再次加入到自己的报文头中，比如指明数据应该发到哪里的路由地址、数据包的长度、协议的版本号，等等，这样封装成 IP 数据包后再发往下一层。
* Device：Device 即网络设备，它是网络访问层中面向系统一侧的接口。不过这里所说的设备，跟物理硬件设备并不是同一个概念，Device 只是一种向操作系统端开放的接口，它的背后既可能代表着真实的物理硬件，也可能是某段具有特定功能的程序代码，比如即使不存在物理网卡，也依然可以存在回环设备（Loopback Device）。许多网络抓包工具，比如tcpdump、Wirshark就是在此处工作的。Device 主要的作用是抽象出统一的界面，让程序代码去选择或影响收发包出入口，比如决定数据应该从哪块网卡设备发送出去；还有就是准备好网卡驱动工作所需的数据。
* Driver：网卡驱动程序（Driver）是网络访问层中面向硬件一侧的接口，网卡驱动程序会通过DMA把主存中的待发送的数据包，复制到驱动内部的缓冲区之中。数据被复制的同时，也会把上层提供的 IP 数据包、下一跳的 MAC 地址这些信息，加上网卡的 MAC 地址、VLAN Tag 等信息，一并封装成为以太帧（Ethernet Frame），并自动计算校验和。而对于需要确认重发的信息，如果没有收到接收者的确认（ACK）响应，那重发的处理也是在这里自动完成的。

### 干预网络通信的 Netfilter 框架
![应用收、发数据包所经过的Netfilter钩子](https://static001.geekbang.org/resource/image/68/e5/688b390674c2de47ce398d2ab8d2a3e5.jpg)

* PREROUTING：来自设备的数据包进入协议栈后，就会立即触发这个钩子。注意，如果 PREROUTING 钩子在进入 IP 路由之前触发了，就意味着只要接收到的数据包，无论是否真的发往本机，也都会触发这个钩子。它一般是用于目标网络地址转换（Destination NAT，DNAT）。
* INPUT：报文经过 IP 路由后，如果确定是发往本机的，将会触发这个钩子，它一般用于加工发往本地进程的数据包。
* FORWARD：报文经过 IP 路由后，如果确定不是发往本机的，将会触发这个钩子，它一般用于处理转发到其他机器的数据包。
* OUTPUT：从本机程序发出的数据包，在经过 IP 路由前，将会触发这个钩子，它一般用于加工本地进程的输出数据包。
* POSTROUTING：从本机网卡出去的数据包，无论是本机的程序所发出的，还是由本机转发给其他机器的，都会触发这个钩子，它一般是用于源网络地址转换（Source NAT，SNAT）。

以 Netfilter 为基础的应用也有很多，其中使用最广泛的毫无疑问要数 Xtables 系列工具，比如iptables、ebtables、arptables、ip6tables，等等。

### 虚拟化网络设备

#### 网卡：tun/tap、veth
tun 和 tap 是两个相对独立的虚拟网络设备，其中 tap 模拟了以太网设备，操作二层数据包（以太帧），tun 则是模拟了网络层设备，操作三层数据包（IP 报文）

使用 tun/tap 设备的目的，其实是为了把来自协议栈的数据包，先交给某个打开了/dev/net/tun字符设备的用户进程处理后，再把数据包重新发回到链路中。这里你可以通俗地理解为，这块虚拟化网卡驱动一端连接着网络协议栈，另一端连接着用户态程序，而普通的网卡驱动则是一端连接着网络协议栈，另一端连接着物理网卡。

VPN 应用程序：
![VPN中数据流动示意图](https://static001.geekbang.org/resource/image/3b/ab/3b88a0148da9a84bd782a7e0a0c384ab.jpg?wh=1504*809)

使用 tun/tap 设备来传输数据需要经过两次协议栈，所以会不可避免地产生一定的性能损耗。

veth 实际上也不是一个设备，而是一对设备，因而它也常被称作 veth pair。我们要使用 veth，就必须在两个独立的网络名称空间中进行才有意义，因为 veth pair 是一端连着协议栈，另一端彼此相连的，在 veth 设备的其中一端输入数据，这些数据就会从设备的另一端原样不动地流出。
![veth pair工作示意图](https://static001.geekbang.org/resource/image/cd/67/cd90382e98805d560810b23f4f273367.jpg?wh=553*336)

两个容器之间采用 veth 通信，不需要反复多次经过网络协议栈，这就让 veth 比起 tap/tun 来说，具备了更好的性能，也让 veth pair 的实现变得十分简单。

#### 交换机：Linux Bridge
二层转发、泛洪、STP、MAC 学习、地址转发表，等等。
它与普通的物理交换机也还是有一点差别的，普通交换机只会单纯地做二层转发，Linux Bridge 却还支持把发给它自身的数据包，接入到主机的三层协议栈中。

![Linux Bridge构建单IP容器网络](https://static001.geekbang.org/resource/image/9b/af/9bb747d54574ee8cbd9c4e22d16124af.jpg?wh=682*534)

#### 网络：VXLAN
软件定义网络（Software Defined Network，SDN）的需求在云计算和分布式时代，就变得前所未有地迫切。SDN 的核心思路是在物理的网络之上，再构造一层虚拟化的网络，把控制平面和数据平面分离开来，实现流量的灵活控制，为核心网络及应用的创新提供良好的平台。

SDN 里，位于下层的物理网络被称为 Underlay，它着重解决网络的连通性与可管理性；位于上层的逻辑网络被称为 Overlay，它着重为应用提供与软件需求相符的传输服务和网络拓扑。以 VXLAN 为例，给你介绍 Overlay 网络的原理。

为了解决 VLAN 的两个明显的缺陷（VLAN Tag 的设计、跨数据中心传递），IETF 定义了 VXLAN 规范，这是三层虚拟化网络（Network Virtualization over Layer 3，NVO3）的标准技术规范之一，是一种典型的 Overlay 网络。

VXLAN 对网络基础设施的要求很低，不需要专门的硬件提供特别支持，只要三层可达的网络就能部署 VXLAN。
VXLAN 带来了很高的灵活性、扩展性和可管理性，VXLAN也带来了额外的复杂度和性能开销（传输效率的下降、传输性能的下降）

#### 副本网卡：MACVLAN
两个 VLAN 之间位于独立的广播域，是完全二层隔离的，要通信就只能通过三层设备。而最简单的三层通信就是靠单臂路由了。单臂路由那把路由器上的两个接口分别设置不同的 IP 地址，然后用两条网线分别连接到交换机上，也勉强算是一个解决办法，但 VLAN 最多可以支持 4096 个 VLAN，那如果要接四千多条网线就太离谱了。因此为了解决这个问题，802.1Q 规范中专门定义了子接口（Sub-Interface）的概念，它的作用是允许在同一张物理网卡上，针对不同的 VLAN 绑定不同的 IP 地址。

MACVLAN 就借用了 VLAN 子接口的思路，并且在这个基础上更进一步，不仅允许对同一个网卡设置多个 IP 地址，还允许对同一张网卡上设置多个 MAC 地址，这也是 MACVLAN 名字的由来。

![MACVLAN原理](https://static001.geekbang.org/resource/image/17/da/177318346dbc945526c064cf618f30da.jpg?wh=1696*1125)

虽然容器间的网络方案多种多样，但通信主体都是固定的，不外乎没有物理设备的虚拟主体（容器、Pod、Service、Endpoints 等等）、不需要跨网络的本地主机、以及通过网络连接的外部主机三种层次。所有的容器网络通信问题，其实都可以归结为本地主机内部的多个容器之间、本地主机与内部容器之间，以及跨越不同主机的多个容器之间的通信问题。

## 6. 容器网络生态 CNM VS CNI
提出容器网络标准的目的，就是为了把网络功能从容器运行时引擎、或者容器编排系统中剥离出去，毕竟网络的专业性和针对性极强，如果不把它变成外部可扩展的功能，而都由自己来做的话，不仅费时费力，还不讨好。

网络的专业性与针对性也决定了 CNM 和 CNI 都采用了插件式的设计，CNM 和 CNI 的网络插件提供的能力，都可以划分为网络的管理与 IP 地址的管理两类，而插件可以选择只实现其中的某一个，也可以全部都实现。

尽管 CNM 规范已是明日黄花，但它作为容器网络的先行者，对后续的容器网络标准的制定仍然有直接的指导意义。

### 管理网络创建与删除
这项能力解决的是如何创建网络、如何将容器接入到网络，以及容器如何退出和删除网络的问题。

### 管理 IP 地址分配与回收
首先是要符合 IPv4 的网段规则，其次是必须考虑到回收的问题，最后还必须要关注时效性。

### 网络插件生态
* Overlay 模式：这是一种虚拟化的上层逻辑网络，好处在于它不受底层物理网络结构的约束，有更大的自由度，更好的易用性；坏处是由于额外的包头封装，导致信息密度降低，额外的隧道封包解包会导致传输性能下降
* 路由模式：它的跨主机通信是直接通过路由转发来实现的，因而不需要在不同主机之间进行隧道封包，路由转发要依赖于底层网络环境的支持
* Underlay 模式：特指让容器和宿主机处于同一网络，两者拥有相同的地位的网络方案

## 7. Kubernetes存储设计

### Mount 和 Volume
Mount 和 Volume 都是来源于操作系统的常用术语，
Mount 是动词，表示将某个外部存储挂载到系统中；
Volume 是名词，表示物理存储的逻辑抽象，目的是为物理存储提供有弹性的分割方式。

### Docker 内建支持了三种挂载类型
Docker 内建支持了三种挂载类型，分别是 Bind（--mount type=bind）、Volume（--mount type=volume）和 tmpfs（--mount type=tmpfs）
* Bind：作用是把宿主机的某个目录（或文件）挂载到容器的指定目录（或文件）下
* Volume：提出 Volume 最核心的一个目的，是为了提升 Docker 对不同存储介质的支撑能力，这同时也是为了减轻 Docker 本身的工作量
* tmpfs：主要用于在内存中读写临时数据

### 持久化的 PersistentVolume 和非持久化的普通 Volume

* Volume：普通 Volume 的设计目标并不是为了持久地保存数据，而是为同一个 Pod 中多个容器提供可共享的存储资源，所以普通 Volume 的生命周期非常明确，也就是与挂载它的 Pod 有着相同的生命周期。
* PersistentVolume：PersistentVolume 可以独立于 Pod 存在，生命周期与 Pod 无关，所以也就决定了 PersistentVolume 不应该依附于任何一个宿主机节点，否则必然会对 Pod 调度产生干扰限制。

### PersistentVolumeClaim 资源
PersistentVolume 是由管理员负责提供的集群存储。PersistentVolumeClaim 是由用户负责提供的存储请求。
Kubernetes 对 PersistentVolumeClaim 与 PersistentVolume 撮合的结果是产生一对一的绑定关系，“一对一”的意思是 PersistentVolume 一旦绑定在某个 PersistentVolumeClaim 上，直到释放以前都会被这个 PersistentVolumeClaim 所独占，不能再与其他 PersistentVolumeClaim 进行绑定。

### Dynamic Provisioning
Dynamic Provisioning 方案是指在用户声明存储能力的需求时，不是期望通过 Kubernetes 撮合来获得一个管理员人工预置的 PersistentVolume，而是由特定的资源分配器（Provisioner）自动地在存储资源池或者云存储系统中分配符合用户存储需要的 PersistentVolume，然后挂载到 Pod 中使用。
![StorageClass运作过程](https://static001.geekbang.org/resource/image/8a/36/8a1a0b9d17cc4ddcf5eee874012b4236.jpg)

### Kubernetes 存储架构
一个真实的存储系统是如何接入到新创建的 Pod 中，成为可以读写访问的 Volume，以及当 Pod 被销毁时，Volume 如何被回收，回归到存储系统之中的。

Kubernetes 其实是参考了传统操作系统接入或移除新存储设备的做法，把接入或移除外部存储这件事情，分解为了以下三个操作：
* 决定应准备（Provision）何种存储
* 将准备好的存储附加（Attach）到系统中
* 将附加好的存储挂载（Mount）到系统中

Kubernetes 目前同时支持FlexVolume与CSI（Container Storage Interface）两套独立的存储扩展机制。FlexVolume 是 Kubernetes 早期版本，目前已经处于冻结状态，可以正常使用但不再发展新功能了。CSI 则是从 Kubernetes 1.9 开始加入的扩展机制，这是目前 Kubernetes 重点发展的扩展机制。

### Kubernetes存储生态系统
目前出现过的存储系统和设备，我们都可以划分到块存储、文件存储和对象存储这三种存储类型之中。

#### 块存储
块存储是数据存储最古老的形式，它把数据都储存在一个或多个固定长度的块（Block）中，想要读写访问数据，就必须使用与存储相匹配的协议（SCSI、SATA、SAS、FCP、FCoE、iSCSI……）。
我们熟悉的硬盘就是最经典的块存储设备，以机械硬盘为例，一个块就是一个扇区，大小通常在 512 Bytes 至 4096 Bytes 之间。
块存储由于贴近底层硬件，没有文件、目录、访问权限等的牵绊，所以性能通常都是最优秀的（吞吐量高，延迟低）。

#### 文件存储
文件存储是最贴近人类用户的数据存储形式，数据存储在长度不固定的文件之中，用户可以针对文件进行新增、写入、追加、移动、复制、删除、重命名等各种操作，通常文件存储还会提供有文件查找、目录管理、权限控制等额外的高级功能。
你可以类比 Linux 下的各种文件管理命令来自行想象一下。
人们把定义文件分配表（File Allocation Table，FAT）应该如何实现、储存哪些信息、提供什么功能的标准称为文件系统（File System），FAT32、NTFS、exFAT、ext2/3/4、XFS、BTRFS 等都是很常用的文件系统。而前面介绍存储插件接口时，我提到的对分区进行高级格式化操作，实际上就是在初始化一套空白的文件系统，供后续用户与应用程序访问。
计算机需要把路径进行分解，然后逐级向下查找，最后才能查找到需要的文件。而要从文件分配表中确定具体数据存储的位置，就要判断文件的访问权限，并要记录每次修改文件的用户与时间，这些额外操作对于性能产生的负面影响也是无可避免的

（考虑到 EFS 的性能、动态弹性、可共享这些因素，我给出的明确建议是它可以作为大部分容器工作负载的首选存储。）

#### 对象存储
对象存储是相对较新的数据存储形式，它是一种随着云数据中心的兴起而发展起来的存储，是以非结构化数据为目标的存储方案。
这里的“对象”可以理解为一个元数据及与其配对的一个逻辑数据块的组合，元数据提供了对象所包含的上下文信息，比如数据的类型、大小、权限、创建人、创建时间，等等，数据块则存储了对象的具体内容。你也可以简单地理解为数据和元数据这两样东西共同构成了一个对象。
对象存储基本上只会在分布式存储系统之上去实现，由于对象存储天生就有明确的“元数据”概念，不必依靠文件系统来提供数据的描述信息，因此，完全可以将一大批对象的元数据集中存放在某一台（组）服务器上，再辅以多台 OSD（Object Storage Device）服务器来存储对象的数据块部分。当外部要访问对象时，多台 OSD 能够同时对外发送数据，因此对象存储不仅易于共享、拥有庞大的容量，还能提供非常高的吞吐量。不过，由于需要先经过元数据查询确定 OSD 存放对象的确切位置，这个过程可能涉及多次网络传输，所以在延迟方面就会表现得相对较差。

## 8. Kubernetes的资源模型与调度器设计
“一切皆为资源”的设计也是 Kubernetes 能够顺利施行声明式 API 的必要前提。

从编排系统的角度来看，Node 是资源的提供者，Pod 是资源的使用者，而调度是将两者进行恰当的撮合。

Node 通常能够提供三方面的资源：计算资源（如处理器、图形处理器、内存）、存储资源（如磁盘容量、不同类型的介质）和网络资源（如带宽、网络地址）。其中与调度关系最密切的是处理器和内存，虽然它们都属于计算资源，但两者在调度时又有一些微妙的差别：
* 处理器这样的资源，被叫做是可压缩资源（Compressible Resources），特点是当可压缩资源不足时，Pod 只会处于“饥饿状态”，运行变慢，但不会被系统杀死，也就是容器会被直接终止，或者是被要求限时退出。
* 而像内存这样的资源，则被叫做是不可压缩资源（Incompressible Resources），特点是当不可压缩资源不足，或者超过了容器自己声明的最大限度时，Pod 就会因为内存溢出（Out-Of-Memory，OOM）而被系统直接杀掉。

### 服务质量与优先级
Kubernetes 给出的配置中有limits和requests两个设置项。
这两者的区别其实很简单：request是给调度器用的，Kubernetes 选择哪个节点运行 Pod，只会根据requests的值来进行决策；而limits才是给 cgroups 用的，Kubernetes 在向 cgroups 的传递资源配额时，会按照limits的值来进行设置。

Kubernetes 会采用这样的设计，完全是基于“心理学”的原因，大多数的工作负载运行过程中，真正使用到的资源，其实都远小于它所请求的资源配额。

#### 服务质量等级
Kubernetes 目前提供的服务质量等级一共分为三级，由高到低分别为 Guaranteed、Burstable 和 BestEffort。
1. 如果 Pod 中所有的容器都设置了limits和requests，且两者的值相等，那此 Pod 的服务质量等级就是最高的 Guaranteed；
2. 如果 Pod 中有部分容器的 requests 值小于limits值，或者只设置了requests而未设置limits，那此 Pod 的服务质量等级就是第二级 Burstable；
3. 如果是前面说的那种情况，limits和requests两个都没设置，那就是最低的 BestEffort 了。

#### 优先级
优先级会影响调度，这很容易理解，这就是说当多个 Pod 同时被调度的话，高优先级的 Pod 会优先被调度。而 Pod 越晚被调度，就越大概率地会因节点资源已被占用而不能成功。


#### 驱逐机制（Eviction）
Kubernetes 就迫不得已要杀掉一部分 Pod，以腾出资源来保证其余 Pod 能正常运行，这个操作就是我后面要给你介绍的驱逐机制（Eviction）。

Pod 的驱逐机制是通过 kubelet 来执行的，kubelet 是部署在每个节点的集群管理程序，因为它本身就运行在节点中，所以最容易感知到节点的资源实时耗用情况。kubelet 一旦发现某种不可压缩资源将要耗尽，就会主动终止节点上服务质量等级比较低的 Pod，以保证其他更重要的 Pod 的安全。而被驱逐的 Pod 中，所有的容器都会被终止，Pod 的状态会被更改为 Failed。

驱逐机制中就有了软驱逐（Soft Eviction）、硬驱逐（Hard Eviction）以及优雅退出期（Grace Period）的概念。
软驱逐是为了减少资源抖动对服务的影响，硬驱逐是为了保障核心系统的稳定，它们并不矛盾，一般会同时使用。

### 默认调度器
调度是为新创建出来的 Pod，寻找到一个最恰当的宿主机节点去运行它。而在这句话里，就包含有“运行”和“恰当”两个调度中的关键过程，它们具体是指：
* 运行：从集群的所有节点中，找出一批剩余资源可以满足该 Pod 运行的节点。为此，Kubernetes 调度器设计了一组名为 Predicate 的筛选算法。
* 恰当：从符合运行要求的节点中，找出一个最适合的节点完成调度。为此，Kubernetes 调度器设计了一组名为 Priority 的评价算法。

#### 共享状态（Shared State）的双循环调度机制
![状态共享的双循环](https://static001.geekbang.org/resource/image/4a/88/4ab5138f0f80db796bc07d5cb1b10d88.jpg)

## 9. 服务网格
服务网格试图以容器、虚拟化网络、边车代理等技术所构筑的新一代通信基础设施为武器，重新对已经盖棺定论三十多年的程序间远程通信中，非透明的原则发起冲击。

### 通信的成本
* 第一阶段：将通信的非功能性需求视作业务需求的一部分，由程序员来保障通信的可靠性。
* 第二阶段：将代码中的通信功能抽离重构成公共组件库，通信的可靠性由专业的平台程序员来保障。
* 第三阶段：将负责通信的公共组件库分离到进程之外，程序间通过网络代理来交互，通信的可靠性由专门的网络代理提供商来保障。（这种思路后来演化出了两种改进形态：第一种形态，微服务网关，将网络代理从进程身边拉远；第二种形态，边车代理，如果将网络代理往进程方向推近。）
* 第四阶段：将网络代理以边车的形式注入到应用容器，自动劫持应用的网络流量，让通信的可靠性由专门的通信基础设施来保障。
* 第五阶段：将边车代理统一管控起来实现安全、可控、可观测的通信，将数据平面与控制平面分离开来，实现通用、透明的通信，这项工作就由专门的服务网格框架来保障。

### 数据平面
数据平面领域已经有了 Linkerd、Nginx、Envoy 等产品。
数据平面由一系列边车代理所构成，它的核心职责是转发应用的入站（Inbound）和出站（Outbound）数据包，因此数据平面也有个别名叫转发平面（Forwarding Plane）。

数据平面至少需要妥善解决三个关键问题：
* 代理注入：边车代理是如何注入到应用程序中的？
* 流量劫持：边车代理是如何劫持应用程序的通信流量的？
* 可靠通信：边车代理是如何保证应用程序的通信可靠性的？

### 控制平面
在控制平面领域也有 Istio、Open Service Mesh、Consul 等产品。
控制平面的特点是不直接参与程序间通信，只会与数据平面中的代理通信。在程序不可见的背后，默默地完成下发配置和策略，指导数据平面工作。如果说数据平面是城市的交通道路，那控制平面就是路口的指示牌与交通信号灯。

1. 数据平面交互：这是部分是满足服务网格正常工作所需的必要工作。
2. 流量控制：这通常是用户使用服务网格的最主要目的。
3. 通信安全：包括通信中的加密、凭证、认证、授权等功能。
4. 可观测性：包括日志、追踪、度量三大块能力。

### 服务网格与生态
服务网格是数据平面产品与控制平面产品的集合，所以在规范制订方面，很自然地也分成了两类：
* SMI 规范提供了外部环境（实际上就是 Kubernetes）与控制平面交互的标准，使得 Kubernetes 及在其之上的应用，能够无缝地切换各种服务网格产品；
* UDPA 规范则提供了控制平面与数据平面交互的标准，使得服务网格产品能够灵活地搭配不同的边车代理，针对不同场景的需求，发挥各款边车代理的功能或者性能优势。

数据平面产品：Linkerd、Envoy、nginMesh、Conduit/Linkerd 2、MOSN
控制平面产品：Linkerd 2、Istio、Consul Connect、Open Service Mesh（OSM）

相当多的程序员已经习惯了通过代码与组件库去进行微服务治理，并且已经积累了很多的经验，也能把产品做得足够成熟稳定，所以对服务网格的需求并不迫切；另一方面，目前服务网格产品的成熟度还有待提高，冒险迁移过于激进，也容易面临兼容性的问题。


